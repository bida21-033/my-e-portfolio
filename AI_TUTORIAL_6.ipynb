{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMcX5tG-XqzM",
        "outputId": "c15554e0-7e74-44f1-f31b-cadce98139e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install necessary libraries (if not already installed)\n",
        "!pip install tensorflow nltk\n",
        "\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ylo2iYFJCi9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (movie reviews)\n",
        "# Here we're assuming the dataset is available as 'movie_reviews.csv'\n",
        "import pandas as pd\n",
        "data = pd.read_csv('IMDB_Dataset.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8NqipvSZGbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into input features (X) and target (y)\n",
        "X = data['review'].values\n",
        "y = data['sentiment'].values  # Assuming sentiment is binary (0 for negative, 1 for positive)"
      ],
      "metadata": {
        "id": "zFQ8FH2VZRrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenize the text data\n",
        "vocab_size = 10000  # Limit the vocabulary size\n",
        "max_len = 200  # Max number of words per review"
      ],
      "metadata": {
        "id": "CEsZEcB3ZhX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "Cea-5Dx0Zlw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "IBsYPgDHZwCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few sequences\n",
        "print(X_train[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxseBGGXZ0wP",
        "outputId": "dd830099-e9d4-4b04-947b-9bb9bbcec292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3015 3757 1882    2  146  145    3  226    4    3  207  326    2  145\n",
            "  1077   16   88    4  132 2872 9046   18   10  154 9889   99    4    1\n",
            "  3898  300   11   17  989   36    1  495  492 2592  250   73   76  106\n",
            "   106  696   60   85 1057 1343    5  229  132   23 4201   31  138  212\n",
            "  1133   14 4511 5339   31    3 2409    2    8   11    6    3  448   14\n",
            "   619    4    1  719 3052    1 1245    2   73 3596    1  164 1514    1\n",
            "  1239    5 1674    1  888 1261    5    1  309  140 2889    2  410  624\n",
            "     7    7    1  272    6 3683 1010    5   26   39   14 1378  217   65\n",
            "     2   46    6   30  224   27  191 1473    8 1088   18   10 4656   84\n",
            "     1  227   66  358   68   54   27    5 3749   15   44   21  193    5\n",
            "  8407    3  879 3438 1773   22   25    5  160  197  176    3  111   12\n",
            "  1569  472   75  221    5  327    2 3498   35   23   51   71 1890 4808\n",
            "    14    9 1388   11   19    6    3 3574 2064   16   61    1 2414  469\n",
            "   533    2    3  171 2643 2811 1776    5  586    9   36    1 3933  454\n",
            "   156  206  352 3944]\n",
            " [   3 4073  202    7    7    1  448    6  330  507   96  249    5   76\n",
            "     1  309    5  423    1  111  184    5   26   32 1386   15  712 1370\n",
            "    36   27  129    5  157  313    6  335    2 1514 3981   38   29   24\n",
            "   685 6354  215  618   14 2126 1370    6  700  248   71  647    3  214\n",
            "     9   22  203   41    4    1   64  252  129    6 1309  718    1   12\n",
            "    80 3728   20    1  268    6  595   14  526   14  252  129    6 1706\n",
            "     8 1317  138   14  439 5146    1 9483 2270   31    1  418  108   23\n",
            "   152   84   22   67   16    1  102 4581    2 2546 1560    2    1  872\n",
            "     1  108  653   53    8    6   21  188  411    1  558   12   28    6\n",
            "  4357   53    8    3 1827  602    5 2479    3  423   36    1  309    7\n",
            "     7   43 1370   66   21  253  138   32 1176  108   35  110   62   16\n",
            "     1   79  102    8    1   17    2  645 4770    6 1099 6494  556   15\n",
            "   264    5 2479  925    2    1  202  134 1190   34  718    2 7542   29\n",
            "     1  616  463  225   96 1077   11   17   98   25   75 1712   18    9\n",
            "   213   89  103    9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common function to build and compile models\n",
        "def build_model(model_type='RNN', bidirectional=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "\n",
        "    if model_type == 'RNN':\n",
        "        model.add(SimpleRNN(128, return_sequences=False))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(LSTM(128, return_sequences=False))\n",
        "\n",
        "    if bidirectional:\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "OGmG1frbZ5RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your labels\n",
        "le.fit(y)\n",
        "\n",
        "# Transform your labels to numerical representations\n",
        "y_num = le.transform(y)\n",
        "\n",
        "# Now use y_num for train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_num, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MaJ1cm_uZ_zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train RNN Model\n",
        "rnn_model = build_model('RNN')\n",
        "history_rnn = rnn_model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEzkpRO8aSiq",
        "outputId": "cef88581-96c6-4980-b7e0-9c73422f46f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 145ms/step - accuracy: 0.5154 - loss: 0.7386 - val_accuracy: 0.7901 - val_loss: 0.4692\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 157ms/step - accuracy: 0.7653 - loss: 0.5068 - val_accuracy: 0.7574 - val_loss: 0.5166\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 151ms/step - accuracy: 0.7694 - loss: 0.4997 - val_accuracy: 0.6286 - val_loss: 0.7996\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 150ms/step - accuracy: 0.7099 - loss: 0.5752 - val_accuracy: 0.8047 - val_loss: 0.4549\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 156ms/step - accuracy: 0.8351 - loss: 0.4070 - val_accuracy: 0.8165 - val_loss: 0.4413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = build_model('LSTM')\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH0672sKcmkk",
        "outputId": "8b65c714-c3b4-47c7-b8f0-4d1f27842494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 554ms/step - accuracy: 0.7635 - loss: 0.4709 - val_accuracy: 0.8798 - val_loss: 0.2962\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 539ms/step - accuracy: 0.9033 - loss: 0.2516 - val_accuracy: 0.8879 - val_loss: 0.2722\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 545ms/step - accuracy: 0.9284 - loss: 0.1891 - val_accuracy: 0.8925 - val_loss: 0.2777\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 538ms/step - accuracy: 0.9457 - loss: 0.1474 - val_accuracy: 0.8804 - val_loss: 0.2949\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 542ms/step - accuracy: 0.9552 - loss: 0.1243 - val_accuracy: 0.8848 - val_loss: 0.3187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common function to build and compile models\n",
        "def build_model(model_type='RNN', bidirectional=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "\n",
        "    if model_type == 'RNN':\n",
        "        model.add(SimpleRNN(128, return_sequences=False))\n",
        "    elif model_type == 'LSTM':\n",
        "        # If bidirectional is True, set return_sequences to True for the LSTM layer\n",
        "        model.add(LSTM(128, return_sequences=bidirectional))\n",
        "\n",
        "    if bidirectional:\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cQxmEHjFjr2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVljtfaGkGe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TUTORIAL 7\n"
      ],
      "metadata": {
        "id": "hyIWl5YqClDD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNATGI2mCnwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}